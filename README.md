# SIGIR'23 Short Paper Data and Reproduction Code 
This repository has the data used to generate the results presented in the paper: 
_Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study._
## ðŸ”– Paper's abstract 
This paper explores the utility of a Large Language Model (LLM) to automatically generate queries and query variants from a description of an information need. Given a set of 100 information needs described as backstories, we explore how similar the queries generated by the LLM are to those generated by humans. We quantify the similarity using different metrics and examine how the use of each set would contribute to document pooling, when building test collections. Our results show potential in using LLMs to generate query variants. While they may not fully capture the wide variety of human-generated variants, they generate similar sets of relevant documents, reaching up to 71.1% overlap at depth 100 during pooling, offering a cost-effective solution for constructing test collections.


## ðŸ“Š Data 
### Query Variants
You can run the script `variant_generation/generate_variants.py` to generate query variants for the backstories provided in the UQV100 using GPT 3.5.
For each backstory, the script builds a prompt using the (A) task description (DESC_A) given in variant_generation/prompts.py, appends (B) a random example, and provide the (C) input backstory.
Note that you need to provide an access key to be able to use the OpenAI API.
Alternatively, you can access the generated query variants used in this paper with varying temperatures at: `gpt_generated_variants/`

### Runs
Runs generated in response to the query variants given the human set and the three GPT sets are available at: `runs/` 

[//]: # ()
[//]: # (## ðŸ¤– Reproduction )

[//]: # (To get the set stats &#40;table 1&#41;)
[//]: # (To reproduce results presented in Fig. 3)
[//]: # (To reproduce evaluation metrics &#40;Table 2&#41;)

[//]: # ()
[//]: # (## Cite the paper)

[//]: # (Marwah Alaofi,Luke Gallagher, Mark Sanderson, Falk Scholer, Paul Thomas. 2023. )